{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis for Movie Review.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbSuA/oXcvnDkCfyRzNBG1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRVithu/Sentiment-Analysis-for-Movie-Reviews/blob/main/Sentiment_Analysis_for_Movie_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH4d1BV-_L8E"
      },
      "source": [
        "#Extracting tar.gz file\n",
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(\"/content/review_polarity.tar.gz\", 'r:gz')\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Momm-jYiCjmb"
      },
      "source": [
        "#To store review(Text) and sentiments\n",
        "review_arr = []\n",
        "status_arr = []"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PnvhhPIcWzy"
      },
      "source": [
        "#Read tha files and store in array\n",
        "import os\n",
        "import io\n",
        "\n",
        "dir = \"/content/txt_sentoken/\"\n",
        "sentiment_arr = ['neg', 'pos']\n",
        "i = 0\n",
        "for status in sentiment_arr:\n",
        "    text_files = os.listdir(dir+status)\n",
        "  \n",
        "    for file_name in text_files:\n",
        "        i=i+1\n",
        "        file_dir = dir+status+'/'+file_name\n",
        "        text_data = list()\n",
        "        try:\n",
        "            file = open(file_dir, \"r\")\n",
        "            file_content = file.read()\n",
        "            status_arr.append(status)\n",
        "            text_data = file_content.split('\\n')\n",
        "            text_data = list(filter(None, text_data))          \n",
        "            review_arr.append(' '.join(text_data[:]))\n",
        "            print('success --> ' + str(i))\n",
        "        except OSError:\n",
        "            print('fail --> ' + str(i))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOYYI6znccc-"
      },
      "source": [
        "# import regex(re) and NLP(nltk) packages \n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99BWm0xSmvnl"
      },
      "source": [
        "# Data Cleaning\n",
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(review_arr)):\n",
        "    # Remove all the special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(review_arr[sentence]))\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foZzuIygc3O6",
        "outputId": "521f5644-7fdc-4957-fe2d-d35c73949588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L8GtR7ucQk4"
      },
      "source": [
        "# Using TfidfVectorizer to convert text review into TF-IDF review vectors.\n",
        "vectorizer = TfidfVectorizer (ngram_range=(1, 2),max_features=2500, min_df=7, max_df=0.8)\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCiOsBH3cq5y"
      },
      "source": [
        "#Dividing Data into Training and Test Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, status_arr, test_size=0.2, random_state=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrTYRCdAdHcq",
        "outputId": "7a9cb896-9a58-4ab2-a373-f432bdbd4423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Training the Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cItNseNHdQqN"
      },
      "source": [
        "#Making Predictions and Evaluating the Model\n",
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36j4RGpLdUN9",
        "outputId": "faf23b4c-7e87-41b2-96e3-3c98a62c3cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Evaluate the performance of the model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[172  28]\n",
            " [ 44 156]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.80      0.86      0.83       200\n",
            "         pos       0.85      0.78      0.81       200\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "0.82\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}